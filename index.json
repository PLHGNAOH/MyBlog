[
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Verifying Large-Scale Amazon S3 Data Integrity with Compute Checksum Operation As data volumes continue to grow, ensuring data integrity has become a critical requirement for organizations across all industries. Traditionally, verifying data integrity required downloading or restoring objects ‚Äî a process that introduces high cost, operational overhead, and significant latency. With the introduction of Amazon S3 Compute Checksum using S3 Batch Operations, organizations can now verify data integrity directly at the storage layer, without retrieving the objects.\nThis capability makes large-scale checksum validation more efficient, cost-effective, and operationally reliable.\nWhy Checksums Matter Checksums detect even the smallest change in data ‚Äî down to a single bit. They serve as the foundation for:\nDetecting corruption during storage or transfer Maintaining the reliability of data pipelines Supporting compliance, audit trails, and fixity workflows Tracking data integrity across its lifecycle S3 supports multiple checksum algorithms including SHA-1, SHA-256, MD5, CRC32, CRC32C, and CRC64NVME, the latter optimized for extremely large datasets due to its high speed and parallelization capabilities.\nPrerequisites Before running Compute Checksum operations, you need:\nPermissions to create and execute S3 Batch Operations jobs An IAM role that S3 Batch Operations can assume Three buckets: data bucket, manifest bucket, and report bucket Amazon Athena access for analyzing report data Separating these buckets improves governance and simplifies access control.\nCompute Checksum Workflow 1. Create and Upload the Manifest The manifest is a CSV file listing the objects (bucket, key, version) for which you want to compute checksums.\nAfter creating it, upload it to the manifest bucket so Batch Operations can read it.\n2. Configure IAM Permissions Required permissions include: Reading source objects Reading the manifest Writing the completion report If objects are encrypted with SSE-KMS, decryption permissions must also be included.\nThe IAM role should be created using the S3 Batch Operations use case for proper compatibility.\n3. Create and Run the Compute Checksum Job To run the job:\nSelect your manifest file Choose Compute Checksum as the operation Choose: Calculation type: Full object Recommended algorithm: CRC64NVME Assign the correct IAM role Run the job and monitor its progress Batch Operations scales seamlessly, allowing checksum validation for millions or billions of S3 objects.\n4. Retrieve the Completion Report Once the job finishes:\nOpen the job details Access the completion report stored in the report bucket Copy the S3 URI to use in Athena Each line of the report includes bucket, key, version, status, and a JSON field containing checksum results.\n5. Analyze the Report with Athena Athena allows you to:\nCreate an external table pointing to the report folder Query for failed tasks or checksum mismatches Filter objects by error codes Flatten JSON fields into a structured CSV for downstream analysis or auditing This enables fast, serverless analysis of checksum results at scale.\nWhen to Use Compute Checksum? Compute Checksum is ideal for:\nLong-term archival data validation Regulatory compliance (HIPAA, FedRAMP, financial audits) Large-scale analytics, ML/AI datasets Massive media assets Backup validation and fixity workflows Any workload that relies on data correctness benefits from automated checksum validation.\nConclusion Amazon S3 Compute Checksum Operation provides an efficient, scalable, and cost-effective solution for verifying data integrity across massive datasets.\nBy eliminating the need to retrieve objects, organizations reduce operational complexity while improving the speed and reliability of data validation workflows.\nCombined with S3 Batch Operations and Athena, Compute Checksum becomes a powerful tool for modern data governance, compliance, and large-scale integrity verification.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "How Strangeworks Uses Amazon Braket to Explore Aircraft Cargo Loading Optimization Quantum computing holds the promise of accelerating solutions for complex industrial optimization problems. In this blog, the Strangeworks team (an AWS partner) evaluates different implementations of the Quantum Approximate Optimization Algorithm (QAOA) for an aircraft cargo loading problem posed by Airbus as part of their Quantum Mobility Challenge.\nThe study compares multiple QAOA variants and benchmarks them on the Rigetti Ankaa-3 QPU available through Amazon Braket.\nBackground: The Aircraft Cargo Loading Problem The Airbus cargo loading task represents a bin-packing optimization problem, common in travel, manufacturing, and logistics.\nThese problems are notoriously hard because the number of possible configurations grows exponentially with the number of containers and spaces.\nQuantum computing may accelerate such optimization tasks, especially through hybrid quantum-classical algorithms like QAOA‚Äîwell-suited to today‚Äôs NISQ hardware.\nAlthough current quantum hardware does not outperform classical state-of-the-art solvers, the study successfully achieved correct solutions on problems up to 80 variables/qubits, demonstrating meaningful progress.\nQAOA Variants Evaluated Strangeworks evaluated three QAOA variants:\nStandard QAOA ‚Äì provided in multiple open-source libraries, including Amazon Braket‚Äôs algorithm library. Relax-and-Round (QRR) QAOA ‚Äì a variant from Rigetti improving classical cost computation. StrangeworksQAOA ‚Äì Strangeworks‚Äô enhanced version featuring improved classical optimization steps. The Strangeworks variant consistently outperformed standard QAOA and improved QRR when applied.\nUsing Braket Hybrid Jobs Through Strangeworks Hybrid algorithms like QAOA require orchestrating quantum and classical compute.\nStrangeworks used Amazon Braket Hybrid Jobs, allowing the entire QAOA workflow to be submitted as a single job. Benefits include:\nReduced queueing overhead Cached circuit compilation, enabling reuse of previously compiled circuits Direct integration through Strangeworks Compute platform Together with the Rigetti Ankaa-3 hardware, this workflow provides a practical route for hybrid optimization at larger scales.\nBenchmarking Approach The benchmark focuses on the Airbus cargo loading problem, modeled as a QUBO (Quadratic Unconstrained Binary Optimization).\nDifferent container counts (n) and space counts (N) produce problem sizes up to 78 variables (which map directly to required qubits).\nStrangeworksQAOA was applied to the encoded QUBO.\nFigure 1 (PDF page 4) compares:\nStandard QAOA StrangeworksQAOA Rigetti‚Äôs QRR QAOA Strangeworks‚Äô improved QRR QAOA The enhanced Strangeworks versions consistently delivered lower cost values and better approximation quality.\nLinear regression was applied to analyze scaling behavior.\nResults show:\nStrangeworksQRR has the gradient closest to the optimal Gurobi classical solver Standard QAOA scales poorly and diverges rapidly StrangeworksQAOA performs well on current sizes but may diverge at larger scales Algorithmic Insights: Why StrangeworksQAOA Performs Better QAOA iterates between:\nPreparing a parameterized quantum circuit Measuring bitstrings Computing classical cost Updating parameters Unlike traditional QAOA (which uses the most probable bitstring), StrangewworksQAOA:\nComputes cost for every measured bitstring Tracks the minimum-cost bitstring across all iterations Returns the best classical solution found Avoids issues where probability distributions flatten in large systems This approach is especially effective because the cargo loading problem requires a single optimal classical solution, not an averaged quantum state.\nResults \u0026amp; Interpretation Benchmarking showed that:\nStrangeworksQAOA significantly outperforms standard QAOA under identical noise conditions StrangeworksQRR achieves the closest match to Gurobi‚Äôs optimal solutions The improvements come entirely from better classical post-processing, with no additional quantum cost The method is robust to noise and scalable to larger systems compared to standard approaches These findings highlight the importance of optimizing classical components in hybrid algorithms.\nConclusion \u0026amp; Outlook This study demonstrates that carefully designed hybrid quantum-classical workflows‚Äîparticularly improvements in the classical evaluation loop‚Äîcan dramatically improve optimization results on current noisy quantum devices.\nAs quantum hardware continues to mature, such optimizations will be key to achieving practical quantum advantage.\nFor further exploration of these algorithms, users can access them via the Strangeworks platform and Amazon Braket.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Validate \u0026amp; Protect Amazon Route 53 DNS Zones and Records Amazon Route 53 plays a foundational role in modern cloud architectures by serving as a scalable and highly available DNS service. Because DNS is the first step in routing traffic across the Internet, any misconfiguration‚Äîintentional or accidental‚Äîcan lead to severe disruptions, financial losses, and major operational issues.\nThis article provides a comprehensive overview of how organizations can safeguard their DNS infrastructure through automation, monitoring, real-time alerting, and multi-layered access control.\nüéØ Objectives This post explains how to:\nAutomate recurring backups of DNS zones and records to Amazon S3. Detect and alert on risky changes in real time. Enforce strict least-privilege IAM permissions. Strengthen organization-wide governance with SCPs. Combined, these practices increase DNS reliability and reduce the chance of human error.\n‚öôÔ∏è Prerequisites To implement the full solution, you need:\nAWS permissions: Route 53, IAM, Lambda, EventBridge, S3, CloudWatch. Existing hosted zones. Working knowledge of IAM policies and scheduled event-driven architecture. 1. Automating DNS Backups Backing up DNS data is crucial for preventing long outages resulting from misconfigurations. Many incidents occur because organizations lack a versioned history of DNS records.\nThe automated backup workflow leverages:\nEventBridge for scheduled triggers AWS Lambda for retrieving DNS data Amazon S3 for centralized, versioned storage üîÑ How the Backup Process Works A scheduled EventBridge rule triggers the backup job. Lambda retrieves all hosted zones and their associated DNS records. Backups are stored in S3 in structured folders by timestamp. Execution details are logged in CloudWatch for auditing and debugging. ‚úîÔ∏è Benefits of automated DNS backups Enables rapid recovery after accidental deletions Provides historical snapshots for auditing Simplifies troubleshooting and incident response Ensures consistent data retention without manual effort 2. Proactive DNS Protection Strategies Backup alone is not enough. DNS must be protected from risky operations before they happen.\nüîî 2.1 Real-Time Deletion Alerts Hosted zone deletion is one of the most disruptive actions in Route 53.\nTo detect such events immediately:\nCloudTrail logs the delete operation EventBridge matches the event SNS sends instant notifications to administrators This rapid alerting enables teams to respond swiftly and prevent widespread impact.\nüîê 2.2 IAM Least Privilege Enforcement Least privilege is essential to minimize dangerous mistakes.\nIAM policies should:\nLimit actions to only what a user or system requires Explicitly deny high-risk operations such as DeleteHostedZone Apply consistently across environments This practice significantly reduces human error and internal misuse.\nüõ°Ô∏è 2.3 Strengthening Governance with SCPs While IAM restricts individual users, Service Control Policies enforce global restrictions across all AWS accounts in an organization.\nKey characteristics of SCPs:\nThey cannot be overridden by IAM. They apply to all member accounts under an AWS Organization or OU. They enforce uniform governance policies across teams. An SCP denying hosted zone deletion ensures critical DNS infrastructure remains protected under all circumstances.\n3. Cleaning Up Resources If running a test deployment, remove:\nLambda functions and CloudWatch logs S3 buckets used for temporary storage EventBridge schedules SNS topics Temporary IAM roles or SCPs This helps avoid unnecessary charges and maintains account hygiene.\nConclusion DNS failures can be catastrophic, but they are also highly preventable.\nBy integrating:\nAutomated backups, Real-time monitoring, Least privilege access controls, and Organization-level enforcement, you can build a resilient DNS architecture that prevents outages, improves operational stability, and safeguards your applications against unintended disruptions.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create a gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your official report, including this warning.\nSummary Report: ‚ÄúAI/ML/GenAI on AWS Workshop‚Äù Event Information Date: Saturday, November 15, 2025 Time: 8:30 AM ‚Äì 12:00 PM Location: AWS Vietnam Office Event Objectives Provide an overview of the AI/ML ecosystem in Vietnam Understand AWS AI/ML services, especially Amazon SageMaker Learn how to build GenAI applications with Amazon Bedrock Master Prompt Engineering, RAG, and Guardrails techniques Explore hands-on demos from AWS experts Agenda \u0026amp; Key Contents 8:30 ‚Äì 9:00 AM | Welcome \u0026amp; Introduction Participant registration and networking Workshop goals and expectations Ice-breaker activities for team engagement Overview of the AI/ML landscape in the Vietnam market 9:00 ‚Äì 10:30 AM | AWS AI/ML Services Overview Amazon SageMaker ‚Äì End-to-End ML Platform Data preparation and labeling Training, tuning, and deploying ML models Managing the full ML lifecycle with integrated MLOps Live Demo Hands-on experience with the interface and workflow in SageMaker Studio 10:30 ‚Äì 10:45 AM | Coffee Break 10:45 AM ‚Äì 12:00 PM | Generative AI with Amazon Bedrock Foundation Models Comparison of Claude, Llama, and Titan Guidance on selecting the right model for specific use cases Prompt Engineering Prompt-writing techniques Chain-of-Thought reasoning Few-shot prompting methods RAG (Retrieval-Augmented Generation) RAG architecture Integrating Knowledge Bases into applications Bedrock Agents Designing multi-step workflows Integrating tools and APIs into task execution Guardrails Ensuring content safety Output filtering based on organizational policies Live Demo Building a GenAI chatbot using Amazon Bedrock Key Takeaways 1. AI/ML on AWS SageMaker provides end-to-end tools for the entire ML lifecycle Standardizing the MLOps process accelerates AI product deployment 2. Generative AI Capabilities Foundation Models reduce training time and cost Prompt engineering greatly influences output quality RAG is ideal for scenarios requiring updated or private data sources 3. Bedrock Ecosystem Bedrock Agents allow complex GenAI applications without heavy backend development Guardrails are essential for maintaining content safety and compliance Event Experience Attending the ‚ÄúAI/ML/GenAI on AWS‚Äù workshop provided valuable insights into both technical solutions and real-world applications.\nLearning from AWS Experts Gained insights into enterprise ML/GenAI programs in Vietnam Understood foundation model options and when to use each one Hands-on Experience SageMaker Studio demo clarified the complete ML workflow Building a GenAI chatbot with Bedrock deepened understanding of RAG architecture and agent-based design Skills Gained Advanced prompt engineering techniques Mindset for designing GenAI applications tailored to business use cases Knowledge of integrating AI into enterprise workflows Networking Connected with AWS experts, solution architects, and the local AI community Discussed real-world cases and practical AI adoption strategies Lessons Learned GenAI must be built on strong data foundations and well-defined guardrails Not every use case requires fine-tuning; choosing the right foundation model is often more effective The combination of SageMaker + Bedrock provides a complete ML/GenAI pipeline for modern applications Event Photos Overall, the workshop helped me gain a deeper understanding of AI/ML/GenAI on AWS and how to apply these technologies effectively in real-world business scenarios. It was a highly valuable experience for developing both technical skills and solution-design thinking.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nStudent Information: Full Name: Ph·∫°m L√™ Huy Ho√†ng\nPhone Number: 0972662408\nEmail: hoangplhse182670@fpt.edu.vn\nUniversity: FPT University\nMajor: AI\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 14/09/2025 to 24/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 24/09/2025 24/09/2025 3 - Learn about AWS Global Infrastructure: Regions, Availability Zones, Edge Locations - Explore AWS Free Tier, billing dashboard, and cost management basics - Practice: + Sign in to AWS Management Console + Enable MFA for root account 25/09/2025 25/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about IAM basics: Users, Groups, Roles, Policies - Understand the principle of least privilege - Practice: + Create an IAM user with programmatic \u0026amp; console access + Attach appropriate policies and test login with IAM user 26/09/2025 26/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2 concepts: + Instance types + AMI + EBS + Security Groups - SSH connection methods to EC2 - Learn about Elastic IP 27/09/2025 27/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice with EC2: + Launch an EC2 instance in a specific region + Configure Security Group (SSH, HTTP/HTTPS if needed) + Connect via SSH using key pair + Attach an EBS volume + Allocate and associate an Elastic IP 28/09/2025 28/09/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nOn this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1:Explore AWS basics: IAM, EC2, Global Infrastructure \u0026amp; basic CLI setup.\nWeek 2:ML basics: Supervised/Unsupervised Learning, Linear Regression, K-means, KNN \u0026amp; Gradient Descent.\nWeek 3: Secure Architectures: IAM, MFA, SCP, Encryption, Security Groups, NACLs, GuardDuty, WAF, Shield \u0026amp; Secrets Manager.\nWeek 4: Resilient Architectures: Multi-AZ, Multi-Region, DR Strategies, Auto Scaling, Route 53, Load Balancing \u0026amp; Backup/Restore.\nWeek 5: Deep Learning Basics: CNN, Preprocessing, Augmentation, Training \u0026amp; Evaluation.\nWeek 6: High-Performing Architectures: EC2 Auto Scaling, Lambda, Fargate, S3/EFS/EBS Performance, Caching, CloudFront, Global Accelerator.\nWeek 7: Cost-Optimized Architectures: Cost Explorer, Budgets, Savings Plans, Lifecycle Policies, NAT Gateway Optimization, Storage Tiering.\nWeek 8: Well-Architected Framework, Core AWS Services: EC2, S3, IAM, RDS, VPC, Lambda, CloudWatch, CloudFront.\nWeek 9: Math for ML/DL, Python Fundamentals, UI Building, Web Apps, REST APIs, Async Programming, Deployment, Classification, Regression, Pipelines, Feature Engineering.\nWeek 10: Foundation Models, Bedrock Agents, RAG Pattern, AI Search Applications, Multi-Modal Models \u0026amp; Fine-Tuning LLMs.\nWeek 11: Serverless Backend, API Frontend, AWS SAM Deployment, Cognito Authentication, Custom Domains \u0026amp; SSL.\nWeek 12: Serverless CRUD, Amplify Auth \u0026amp; Storage, API Gateway Integration, SAM Deployment \u0026amp; CloudFront Delivery.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "AWS First Cloud AI Journey ‚Äì Project Plan Online Shopping Website: Furious Five Fashion (FFF) AWS \u0026amp; AI-Powered E-commerce Website Solution 1. Background and Motivation 1.1 Executive Summary The client is a small-sized business specializing in fashion products for young customers. They aim to build an online clothing e-commerce website using AWS and AI, with the ability to scale flexibly, support long-term growth, and optimize operational costs.\nThe goal of this project is to shift from traditional manual management on physical servers to a flexible, intelligent, and cost-efficient cloud-based model. AWS enables the system to scale at any time, maintain fast access speed, and allow the business to focus on product development instead of infrastructure.\nThe system is designed to support end-to-end e-commerce operations: hosting and distributing web content, managing product and order databases, supporting payments, and monitoring system performance. Everything aims toward stability, security, and long-term scalability.\nThe Furious Five implementation team will accompany the client throughout the process‚Äîadvising, designing the architecture, and configuring key AWS services such as Lambda, S3, DynamoDB, CloudFront, and Route 53. Beyond building the system, they also help optimize costs, ensure security, and train the internal team to manage the infrastructure effectively.\nThis project is not just a technical plan‚Äîit marks an important step in the company‚Äôs digital transformation journey.\n1.2 Project Success Criteria To ensure the success of the Furious Five Fashion project, the following clear and measurable criteria must be met, representing both business goals and technical effectiveness:\nSystem Performance The website must maintain response times under 2 seconds for all user actions, even during peak hours.\nAvailability The system must achieve 99.9% uptime, monitored and automatically reported through services like CloudWatch.\nScalability AWS infrastructure must scale automatically when traffic increases by at least 2√ó without causing service disruption.\nCost Optimization Monthly operating costs must remain under 30% of the projected budget, supported by AWS cost-monitoring tools such as Cost Explorer and Trusted Advisor.\nSecurity No data leaks or unauthorized access. All customer data must be protected by AWS security standards (IAM policies, encryption, HTTPS, etc.).\nDeployment \u0026amp; Operations Infrastructure must be fully deployed within 4 weeks, with complete documentation so the internal team can manage the environment effectively.\nTraining \u0026amp; Knowledge Transfer The internal technical team must be trained to confidently maintain, monitor, and secure the system without depending entirely on external support.\n1.3 Assumptions To ensure alignment and smooth execution of the FFF project, the following assumptions have been made:\nThe team already has access to AWS accounts with required permissions and has basic knowledge of essential AWS services such as Lambda, S3, IAM, and Route 53. Stable Internet connectivity is assumed since all infrastructure runs in the cloud. The team is also aware of basic security and compliance requirements before deployment.\nThe project depends on multiple external factors: stable service availability in the selected AWS region, smooth domain routing via Route 53, and effective collaboration between development teams to ensure the web application operates properly in the cloud environment.\nThe project is part of an internship, so the budget is limited‚Äîfavoring free-tier usage and low-cost service configurations. Due to limited experience and tight timelines, the chosen architecture remains simple and practical.\nPotential risks include IAM misconfigurations, accidental overspending due to unused resources, AWS regional outages, service incompatibilities, or limited expertise in troubleshooting cloud systems.\nDespite these risks, the project is built on clear expectations: this is a pilot environment, with layered monitoring, backup, and cost-management strategies in place. Every challenge is considered an opportunity to learn and grow in cloud engineering.\n2. SOLUTION ARCHITECTURE 2.1 Technical Architecture Diagram The following architecture is designed for FFF, deployed in AWS Region Singapore (ap-southeast-1). It emphasizes flexibility, security, automation, scalability, and simplicity‚Äîappropriate for an internship-level project while following AWS best practices.\nThe system follows a multi-layer design consisting of six key components:\nFrontend \u0026amp; Security Layer Users access the website through Route 53. Incoming traffic is protected with AWS WAF and optimized via CloudFront CDN. Source code is managed and deployed through GitLab CI/CD using CloudFormation templates.\nAPI \u0026amp; Compute Layer API Gateway routes all requests to AWS Lambda, which handles application logic. Cognito manages authentication and access control.\nStorage Layer Two S3 buckets store static content (StaticData) and user uploads.\nData Layer DynamoDB stores product metadata and unstructured data. IAM ensures secure interactions between components.\nAI Layer Amazon Rekognition and Amazon Bedrock power image processing and generative AI features.\nObservability \u0026amp; Security Layer CloudWatch, SNS, and SES provide monitoring, alerting, and system notifications.\n2.2 Technical Implementation Plan Infrastructure will be managed and deployed using Infrastructure as Code (IaC) with AWS CloudFormation to ensure repeatability, stability, and ease of maintenance.\nKey AWS components‚ÄîS3, Lambda, API Gateway, DynamoDB, Cognito, and CloudWatch‚Äîwill be defined entirely through CloudFormation templates stored in GitLab for version control and rollback capability.\nSensitive configurations such as IAM permissions or WAF rules require approval before deployment and follow the internal governance process with review and validation.\nAll critical system paths‚Äîfrom authentication to data processing‚Äîare covered by automated and manual test cases to ensure stability, security, and scalability.\nThis technical plan enables the FFF team to deploy and manage a professional cloud environment, learning real DevOps and AWS best practices.\n2.3 Project Plan The project follows Agile Scrum over 3 months, divided into 4 sprints.\nSprint Structure\nSprint Planning\nSetup AWS foundational services (S3, Route 53, IAM)\nConfigure security (WAF, CloudFront)\nIntegrate backend (Lambda, API Gateway, DynamoDB)\nTesting, optimization, and demo preparation\nDaily Stand-up 30-minute updates to address blockers and track status.\nSprint Review Review deliverables, demo on real AWS environment, fix issues.\nRetrospective Improve DevOps workflows and automation pipeline.\nTeam Roles\nProduct Owner: Business alignment, backlog prioritization\nScrum Master: Coordination, Agile process enforcement\nDevOps/Technical Team: Backend, infrastructure, CI/CD\nMentor / AWS Partner: Architecture validation, AI testing, cost \u0026amp; security review\nCommunication Rhythm\nDaily Stand-ups (23:00)\nWeekly Sync\nEnd-of-Sprint Demo\nKnowledge Transfer After the final sprint, the technical team will deliver hands-on training on operations, monitoring (Budgets, CloudWatch), scaling, and recovery procedures.\n2.4 Security Considerations Access Management MFA for admin users; IAM roles with least privilege; auditing through CloudTrail.\nInfrastructure Security\nEven without a dedicated VPC, services are restricted using resource policies; all public endpoints use HTTPS.\nData Protection\nS3 and DynamoDB encryption; TLS data transfer; manual periodic backups.\nDetection \u0026amp; Monitoring\nCloudTrail, Config, and CloudWatch for visibility; GuardDuty for threat detection.\nIncident Response\nClear incident workflows with log collection, analysis, and periodic simulations.\n3. PROJECT ACTIVITIES \u0026amp; DELIVERABLES 3.1 Activities \u0026amp; Deliverables Table Phase Timeline Activities Deliverables Effort(day) Infrastructure Setup Week 1 ‚Äì 2 Requirements gathering, architecture design, AWS configuration (S3, CloudFront, API, Lambda, DynamoDB, Cognito), GitLab CI/CD setup Completed AWS Architecture, Ready Infrastructure, Active CI/CD 10 Frontend Development Week 3‚Äì5 UI/UX design, FE pages (Home, Catalog, Product Detail, Cart, Checkout), API integration Completed FE (Dev), Frontend connected to API 15 Backend \u0026amp; Database Week 6‚Äì9 Lambda APIs, DynamoDB setup, order/user/product logic, Cognito IAM setup Stable API, validated data flow, full FE-BE integration 20 Testing \u0026amp; Validation Week 10‚Äì11 Functional, security, performance testing, integration testing Test Report, Validated System 5 Production Launch Week 12 Deploy to production, domain \u0026amp; SSL setup, training \u0026amp; handover Live FFF Website, Documentation Package 5 3.2 Out of Scope Mobile applications (iOS/Android)\nReal inventory/logistics integration\nAdvanced admin dashboards\nCRM/ERP integrations\nAdvanced AWS security services\nReal payment gateway integration\nMultilanguage \u0026amp; multicurrency\n3.3 Go-Live Roadmap Phase 1 ‚Äì POC Basic FE, S3 hosting, API integration, sample data storage, CloudFront optimization.\nPhase 2 ‚Äì UAT Cognito auth, sandbox payment, CloudWatch monitoring, internal user testing.\nPhase 3 ‚Äì Production Deployment Route 53 domain setup, SSL via ACM, WAF protection, CloudFront refinement.\nPhase 4 ‚Äì Stabilization \u0026amp; Optimization Cost optimization, performance improvements, backup strategy, documentation updates.\n4. AWS COST ESTIMATION Estimated monthly cost: $30‚Äì35 USD\nRoute 53 : $1.00 AWS WAF : $5.00 CloudFront: $3.90 S3 (StaticData) : $0.50 S3 (Uploads): $0.75 S3 (Bucket): $0.75 AWS Lambda: $0.25 API Gateway: $3.50 Amazon Bedrock: $3.00 DynamoDB: $1.00 IAM: Free CloudWatch: $2.00 SNS: $0.10 SES: $0.20 CloudFormation: Free GitLab CI/CD : $3.00 WS Config / Setup \u0026amp; Test migration tools $5.00 (1 l·∫ßn) Estimated monthly total cost: ~ $30.00 ‚Äì $35.00 USD Cost assumptions:\nRegion: Singapore\n500‚Äì1000 users/month\nLow traffic\nData \u0026lt; 100GB\nFree Tier active for 12 months\nCost optimizations recommended:\nS3 Intelligent-Tiering\nCloudWatch log retention 14‚Äì30 days\nAWS Budgets alert at $40\nConsider Lambda Savings Plan for long-term workloads\n5. Project Team Project Stakeholders Name: Van Hoang Kha Title: Support Teams Description: is the Executive support person responsible for overall supervision of the FCJ internship program\nEmail/Contact information: Khab9thd@gmail.com\nPartner Project Team (Furious Five Internship Team)\nName: Duong Minh Duc Title: Project Team Leader\nDescription: Manage progress, coordinate work between the team and mentor, Manage AWS infrastructure deployment (S3, Lambda, IAM)\nEmail/Contact information: ducdmse182938@fpt.edu.vn\nName: Quach Nguyen Chi Hung\nTitle: Member\nDescription: In charge of UI/UX and user interface\nEmail/Contact information: bacon3632@gmail.com\nName: Nguyen Tan Xuan\nTitle: Member\nDescription: Responsible for Backend and server logic processing\nEmail/Contact information: xuanntse184074@fpt.edu.vn\nName: Nguyen Hai Dang\nTitle: Member\nDescription: Manage AWS infrastructure deployment (S3, Lambda, IAM) and AI chat bot integration\nEmail/Contact information: dangnhse184292@fpt.edu.vn\nName: Pham Le Huy Hoang\nTitle: Member\nDescription: Testing, quality assurance and GitLab CI/CD integration, and AI chat bot integration\nEmail/Contact information: hoangplhse182670@fpt.edu.vn\nContact Complaints / Escalation Project Project\nName: Duong Minh Duc\nTitle: Project Team Leader\nDescription: Represent the internship team to contact the mentor and sponsor directly\nEmail/Contact information: ducdmse182938@fpt.edu.vn\n6. RESOURCES \u0026amp; COST ESTIMATES Resources Roles Responsibilities Rate (USD)/Hour Solution Architect(1) Design overall solutions, ensure technical feasibility, select appropriate AWS services 35 Cloud Engineer(2) Implement AWS infrastructure, configure services (S3, IAM\u0026hellip;), test and optimize systems 20 Project Manager (1) Monitor progress, coordinate teams, manage project scope and risks. 15 Support / Documentation (1) Prepare handover documents, user manuals, and final reports. 10 Estimate costs by project phase Project phase Solution Architect (hrs) 2 Engineers (hrs) Project Manager (hrs) Project Management / Support (hrs) Total Hours Survey \u0026amp; Solution Design 53 40 13 13 119 Implementation \u0026amp; Testing 67 160 21 19 267 Support / Documentation 27 53 21 19 120 Total Hours 147 253 55 51 506 Total Amount $5145 $5060 $825 $510 $11540 Cost Contribution Allocation Party Contribution (USD) % Contribution Customers 4616 40% Partners (Furious Five) 2308 20% AWS 4616 40% "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference only. Please do not copy it verbatim into your official report, including this warning.\nSummary Report: ‚ÄúDevOps on AWS Workshop‚Äù Event Information Date: Monday, November 17, 2025 Time: 8:30 AM ‚Äì 5:00 PM Location: AWS Office Event Objectives Understand key DevOps principles, culture, and mindset Explore AWS DevOps services and build a complete CI/CD pipeline Learn Infrastructure as Code (IaC) with CloudFormation and AWS CDK Gain hands-on knowledge with AWS container services (ECR, ECS, EKS, App Runner) Apply monitoring \u0026amp; observability best practices using CloudWatch and X-Ray Study modern DevOps best practices, deployment strategies, and real-world case studies Agenda \u0026amp; Key Contents 8:30 ‚Äì 9:00 AM | Welcome \u0026amp; DevOps Mindset Recap of previous AI/ML session DevOps culture: collaboration, automation, continuous improvement Key DevOps metrics: DORA metrics, MTTR, deployment frequency Why DevOps increases agility, speed, and reliability 9:00 ‚Äì 10:30 AM | AWS DevOps Services ‚Äì CI/CD Pipeline Source Control AWS CodeCommit overview Git strategies: GitFlow, Trunk-Based Development Build \u0026amp; Test CodeBuild configuration Automated testing pipelines Deployment CodeDeploy with Blue/Green, Canary, and Rolling deployments Orchestration CodePipeline automation for end-to-end CI/CD Live Demo Full CI/CD pipeline walkthrough on AWS 10:30 ‚Äì 10:45 AM | Break 10:45 AM ‚Äì 12:00 PM | Infrastructure as Code (IaC) AWS CloudFormation Templates, stacks, and change sets Drift detection for configuration integrity AWS CDK Constructs, reusable patterns, modular infrastructure Multi-language support: Python, TypeScript, Java, etc. Live Demo Deploying infrastructure using both CloudFormation and CDK Discussion When to choose CloudFormation vs CDK in real projects 12:00 ‚Äì 1:00 PM | Lunch Break (Self-arranged) Afternoon Session (1:00 ‚Äì 5:00 PM) 1:00 ‚Äì 2:30 PM | Container Services on AWS Docker Fundamentals Microservices architecture Containerization workflow Amazon ECR Image storage \u0026amp; scanning Lifecycle policies Amazon ECS \u0026amp; EKS Deployment strategies Auto scaling and orchestration Differences between ECS vs EKS AWS App Runner Simplified deployment of containerized applications Demo \u0026amp; Case Study Comparing microservices deployment using ECS, EKS, and App Runner 2:30 ‚Äì 2:45 PM | Break 2:45 ‚Äì 4:00 PM | Monitoring \u0026amp; Observability Amazon CloudWatch Metrics, dashboards, alarms Log Insights for troubleshooting AWS X-Ray Distributed tracing Visualizing performance bottlenecks Live Demo Setting up a full observability solution Best Practices On-call readiness Alert routing Dashboard design for SRE/DevOps teams 4:00 ‚Äì 4:45 PM | DevOps Best Practices \u0026amp; Case Studies Feature flags, A/B testing, progressive delivery Automated testing and CI/CD integration Incident management \u0026amp; postmortem culture Real-world DevOps transformations (startup vs enterprise) 4:45 ‚Äì 5:00 PM | Q\u0026amp;A \u0026amp; Wrap-up DevOps career growth paths AWS certification roadmap (DevOps Engineer, SAA, SysOps, Solutions Architect Pro) Key Takeaways 1. DevOps Mindset Collaboration between Dev + Ops reduces friction and speeds delivery DORA metrics provide a measurable framework for improvement Culture is as important as tooling 2. CI/CD with AWS CodePipeline + CodeBuild + CodeDeploy form a fully automated release pipeline Deployment strategies like Blue/Green and Canary reduce downtime and risk 3. Infrastructure as Code CDK accelerates development with reusable components CloudFormation ensures stable, consistent environments 4. Container Orchestration ECS offers simplicity, EKS offers flexibility and ecosystem integration App Runner is ideal for rapid, low-maintenance deployment 5. Observability CloudWatch and X-Ray help detect issues early Improving MTTR requires proper alerting and traceability Event Experience Attending the ‚ÄúDevOps on AWS‚Äù workshop provided practical, hands-on knowledge for building modern cloud-native systems.\nLearning from AWS Experts Gained insights into scalable CI/CD pipelines and DevOps workflows Understood differences between container orchestration services and when to use each Hands-on Exposure Built a complete CI/CD pipeline using the AWS DevOps toolchain Practiced deploying infrastructure using both CloudFormation and CDK Explored real microservices deployments on ECS, EKS, and App Runner Skills Gained Designing IaC-based architectures Implementing observability best practices Applying DevOps principles to improve system reliability and automation Networking Opportunities Connected with DevOps engineers, cloud architects, and AWS specialists Discussed real-world challenges such as scaling, incident response, and release strategies Lessons Learned Automation minimizes human error and increases deployment frequency IaC is essential for consistency and repeatability DevOps success requires aligning culture, processes, and tools Event Photos Overall, the workshop enhanced my understanding of DevOps practices on AWS and provided a strong foundation for building automated, scalable, and reliable cloud systems.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Test the Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 2 Objectives: Learn foundational concepts in Machine Learning. Understand both supervised and unsupervised learning methods. Study key ML algorithms including Linear Regression, K-means, KNN, and Gradient Descent. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study Supervised Learning - Study Unsupervised Learning - Understand key differences between the two learning approaches 01/10/2025 01/10/2025 3 - Learn about Linear Regression - Understand cost function, line fitting, and optimization concept 02/10/2025 02/10/2025 4 - Study K-means Clustering - Learn centroid initialization, distance measurement, cluster assignment, and convergence process 03/10/2025 03/10/2025 5 - Study K-Nearest Neighbors (KNN) - Understand distance metrics (Euclidean, Manhattan) - Learn how KNN performs classification and regression 04/10/2025 04/10/2025 6 - Study Gradient Descent - Learn update rule, learning rate, convergence behavior, and common issues (local minima, overshooting) 05/10/2025 05/10/2025 Week 2 Achievements: Understood the difference between supervised and unsupervised learning. Built foundation knowledge of Linear Regression and its optimization. Learned how K-means performs clustering through iterative updates. Understood the working principle of KNN and its use in classification/regression. Studied Gradient Descent and how it is used to optimize machine learning models. Established a strong foundation for progressing to more advanced ML algorithms in later weeks. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: ‚ÄúAWS Well-Architected Security Pillar Workshop‚Äù Event Objectives Understand AWS Well-Architected Security Pillar fundamentals Learn best practices for identity \u0026amp; access management Improve knowledge of data protection methods Identify common cloud security risks and mitigation techniques Explore real-world use cases of implementing security on AWS Speakers AWS Security Specialist Team ‚Äì Vietnam Office Senior Solutions Architects ‚Äì AWS SEA Region Key Highlights Identity \u0026amp; Access Management (IAM) Principle of Least Privilege Multi-Factor Authentication (MFA) enforcement Strong password policies \u0026amp; IAM role separation Infrastructure Protection Use of Security Groups and Network ACLs Protecting workloads across multiple network layers Setting up AWS WAF \u0026amp; AWS Shield for application-level protection Data Protection Encryption using KMS ‚Äì keys, rotations, and policies Protecting data at rest \u0026amp; in transit Using Secrets Manager to securely store and rotate credentials Threat Detection \u0026amp; Monitoring Amazon GuardDuty for continuous threat detection AWS CloudTrail for audit logging CloudWatch Alarms for anomaly detection Key Takeaways Security Mindset Security must be treated as a continuous process, not a one-time setup Strong identity management reduces the risk of account compromise Importance of well-defined boundaries in cloud networking Technical Best Practices Enforce MFA for all IAM users Rotate KMS keys \u0026amp; secrets regularly Use AWS WAF to block common attack patterns Apply network segmentation to limit blast radius Organizational Impact Following the AWS Security Pillar reduces operational risks Improves compliance with business and regulatory requirements Enhances customer trust by protecting sensitive information Applying to Work Implement MFA \u0026amp; least-privilege IAM roles for internal projects Use KMS to encrypt S3 buckets and RDS databases Set up GuardDuty + CloudTrail to monitor suspicious activities Apply WAF rules to protect web applications from common threats Store API keys and database credentials in Secrets Manager Event Experience Attending the AWS Well-Architected Security Pillar Workshop provided valuable insights into securing cloud architectures effectively.\nLearning from AWS Specialists Walkthroughs of real attack scenarios and how AWS tools mitigate them Best practices for IAM, networking, and data protection Hands-on Exposure Simulated exercises for threat detection using GuardDuty Practical guidance on configuring WAF rules and CloudTrail logging Increased Security Awareness Clear understanding of shared responsibility model How to design secure, resilient multi-layer cloud systems Networking \u0026amp; Discussions Engaged in discussions with AWS security experts Learned practical do‚Äôs \u0026amp; don‚Äôts from real-world deployments Lessons Learned Security must evolve as workloads scale Automated monitoring is essential for modern cloud environments Structured frameworks like Well-Architected help maintain consistency Event Photos Overall, the workshop strengthened my understanding of cloud security and equipped me with actionable practices to enhance the security posture of AWS workloads.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Verifying Amazon S3 data integrity at scale with the compute checksum operation This article introduces the new compute checksum operation feature in Amazon S3 Batch Operations, which enables organizations to verify the integrity of archived data sets at scale efficiently and cost-effectively.\nBlog 2 - How Strangeworks is using Amazon Braket to explore the aircraft cargo loading problem This article evaluates the implementation of various variations of the Quantum Approximate Optimization Algorithm (QAOA) to solve the aircraft cargo loading problem posed by Airbus, which is a type of bin packing optimization problem.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 3 Objectives: Understand AWS Security Architecture. Learn and practice IAM, MFA, SCP. Learn encryption services: KMS, ACM/TLS. Learn VPC Security: Security Groups \u0026amp; NACLs. Explore threat detection \u0026amp; protection: GuardDuty, Shield, WAF. Manage sensitive data using Secrets Manager. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn IAM: + Users, Groups, Roles + IAM Policies (JSON) + Permission Boundaries - Enable and configure MFA for IAM users 09/10/2025 09/10/2025 https://docs.aws.amazon.com/iam/ 3 - Learn Service Control Policies (SCP) in AWS Organizations - Practice: Create sample SCP, restrict regions, and apply to test OU 10/10/2025 10/10/2025 https://docs.aws.amazon.com/organizations/ 4 - Learn Encryption in AWS: + AWS KMS (CMK types, key policies) + Envelope Encryption - Learn about TLS/SSL certificates with AWS Certificate Manager (ACM) 11/10/2025 11/10/2025 https://docs.aws.amazon.com/kms/ 5 - Learn VPC Security: + Security Groups (stateful) + Network ACLs (stateless) - Compare SG vs NACL - Practice: Configure SG \u0026amp; NACL for a web application 12/10/2025 12/10/2025 https://docs.aws.amazon.com/vpc/ 6 - Learn Threat Detection \u0026amp; Protection: + AWS GuardDuty + AWS Shield (Standard vs Advanced) + AWS WAF rules \u0026amp; ACLs 13/10/2025 13/10/2025 https://docs.aws.amazon.com/waf/ Week 3 Achievements: Configured IAM users, groups, and roles and applied MFA. Created and applied SCPs within AWS Organizations. Understood and used AWS KMS for key management and ACM for TLS certificates. Secured network access using Security Groups and NACLs. Learned how to detect and mitigate threats using GuardDuty, Shield, and WAF. Managed application secrets securely using AWS Secrets Manager. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nIn this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3‚Ä¶, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event‚Äôs content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AI/ML/GenAI on AWS Workshop\nDate \u0026amp; Time: 8:30 AM ‚Äì 12:00 PM, November 15, 2025\nLocation: AWS Vietnam Office, Bitexco Financial Tower, 2 H·∫£i Tri·ªÅu, B·∫øn Ngh√© Ward, District 1, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: DevOps on AWS\nDate \u0026amp; Time: 8:30 AM ‚Äì 5:00 PM, Monday, November 17, 2025\nLocation: AWS Vietnam Office, Bitexco Financial Tower, 2 H·∫£i Tri·ªÅu, B·∫øn Ngh√© Ward, District 1, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AWS Well-Architected Security Pillar\nDate \u0026amp; Time: 08:30 AM ‚Äì 12:00 PM, November 29, 2025\nLocation: AWS Vietnam Office, Bitexco Financial Tower, 2 H·∫£i Tri·ªÅu, B·∫øn Ngh√© Ward, District 1, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 4 Objectives: Understand the design of Resilient \u0026amp; Highly Available Architectures on AWS. Apply core AWS services such as Multi-AZ, Multi-Region, Auto Scaling, ELB, Route 53, Backup \u0026amp; Restore, DR strategies. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about Multi-AZ architectures - Understand AZ-level failover \u0026amp; high availability patterns 14/10/2025 14/10/2025 https://cloudjourney.awsstudygroup.com/ 3 - Study Multi-Region architectures - Learn latency-based routing \u0026amp; global fault tolerance 15/10/2025 15/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn Disaster Recovery (DR) strategies: Backup \u0026amp; Restore, Pilot Light, Warm Standby, Multi-Site 16/10/2025 16/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn Auto Scaling \u0026amp; Elastic Load Balancing (ALB, NLB) - Scaling policies: Target tracking, Step scaling, Scheduled scaling 17/10/2025 17/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn Route 53: Routing policies (Simple, Failover, Weighted, Geolocation, Latency-based) - Study Backup \u0026amp; Restore best practices 18/10/2025 18/10/2025 https://cloudjourney.awsstudygroup.com/ Week 4 Achievements: Understood how to design highly available and fault-tolerant architectures using Multi-AZ and Multi-Region. Studied multiple DR strategies and understood when to apply each: Backup \u0026amp; Restore Pilot Light Warm Standby Multi-Site Active-Active Gained a clear understanding of Auto Scaling, scaling policies, and how to combine scaling with Load Balancers for elastic workloads. Learned how Route 53 provides global traffic control, resilience, and routing optimization. Practiced designing backup plans, recovery objectives (RTO/RPO), and how AWS Backup supports cross-region protection. Developed the ability to combine all these components into a complete Resilient Architecture. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference only. Please do not copy verbatim for your official report, including this warning.\nWeek 5 Objectives: Understand the fundamentals of Deep Learning for image classification. Learn the end-to-end workflow of building a CNN model: preprocessing ‚Üí model design ‚Üí training ‚Üí evaluation. Become familiar with Google Colab and GPU environments. Practice using core libraries such as Numpy, Pandas, Matplotlib, and TensorFlow/PyTorch. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review programming tools for Deep Learning: + Python basics + Numpy, Pandas (numerical data processing) + Matplotlib/Seaborn (visualization) + Working with Google Colab and GPU runtime 20/10/2025 20/10/2025 TensorFlow Docs, PyTorch Docs 3 - Perform image preprocessing: + Resize images + Normalization (0‚Äì1 scaling) + Label encoding (One-hot or Integer) + Split dataset into Train ‚Äì Validation ‚Äì Test 21/10/2025 21/10/2025 Kaggle Docs 4 - Learn Data Augmentation techniques: + Rotation, flipping, zoom, shifting + Generate additional training samples - Visualize transformations using Matplotlib 22/10/2025 22/10/2025 ImageDataGenerator (TensorFlow) 5 - Study CNN architecture: + Conv2D, MaxPooling2D + Flatten, Dense layers + Activation functions (ReLU, Softmax) - Build a basic CNN model on Google Colab 23/10/2025 23/10/2025 DeepLearning.ai 6 - Learn training mechanics: + Loss function (Categorical Crossentropy) + Optimizers (Adam, SGD) + Hyperparameters (Epochs, Batch size, Learning rate) - Evaluate the model using Accuracy \u0026amp; Confusion Matrix - Understand Overfitting \u0026amp; apply Dropout to reduce it 24/10/2025 24/10/2025 Machine Learning Mastery Week 5 Achievements: Gained proficiency with Deep Learning tools: Python, Colab, Numpy‚ÄìPandas‚ÄìMatplotlib, TensorFlow/PyTorch. Completed preprocessing workflow: resize ‚Üí normalize ‚Üí encode ‚Üí split dataset. Applied Data Augmentation to enhance dataset diversity and reduce overfitting. Understood the structure and purpose of CNN layers and activation functions. Learned to choose appropriate loss functions, optimizers, and hyperparameters. Successfully evaluated models using Accuracy, Precision, Recall, and Confusion Matrix. Understood overfitting and implemented solutions such as Dropout and Augmentation. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to gain hands-on experience and apply academic knowledge to real workplace scenarios. Throughout this period, I participated in [briefly describe the main project/task you worked on], which allowed me to strengthen my abilities in areas such as [programming, analysis, reporting, communication, teamwork, etc.].\nI consistently strived to complete assigned tasks to the best of my ability, followed workplace regulations, and maintained a proactive learning attitude. Additionally, I actively collaborated with colleagues, sought feedback, and adjusted my approach to improve work quality.\nTo objectively evaluate my performance during the internship, I provide the following self-assessment:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, ability to apply knowledge, proficiency with tools, work output ‚úÖ ‚òê ‚òê 2 Ability to learn Ability to absorb new concepts, self-study, and adapt to new tools or processes ‚òê ‚úÖ ‚òê 3 Proactiveness Initiative in taking tasks, seeking solutions, minimizing dependency on instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, regulations, and company processes ‚òê ‚òê ‚úÖ 6 Progressive mindset Willingness to accept feedback, improve, and enhance personal performance ‚òê ‚úÖ ‚òê 7 Communication Ability to present ideas, report tasks clearly, and coordinate with others ‚òê ‚úÖ ‚òê 8 Teamwork Collaboration, knowledge sharing, and supporting team members ‚úÖ ‚òê ‚òê 9 Professional conduct Respectful attitude, workplace etiquette, and professional behavior ‚úÖ ‚òê ‚òê 10 Problem-solving skills Ability to identify issues, propose solutions, and demonstrate creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Effectiveness of contributions, initiative, and recognition from mentors or team members ‚úÖ ‚òê ‚òê 12 Overall General evaluation of performance throughout the internship ‚úÖ ‚òê ‚òê Needs Improvement Work on building stronger discipline and maintaining consistency in meeting expectations and workflow requirements. Develop a more methodical approach to analyzing problems and formulating solutions. Improve clarity and confidence in communication, especially in presenting information and managing professional interactions. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: This content is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 6 Objectives: Understand and design High-Performing Architectures on AWS. Learn how to maximize application performance, scalability, and elasticity. Work with AWS services such as EC2 Auto Scaling, Lambda, Fargate, CloudFront, Global Accelerator, S3, EFS, EBS, and caching mechanisms. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduction to High-Performing Architectures - Analyze vertical vs horizontal scaling - Elasticity \u0026amp; Scalability concepts 25/10/2025 25/10/2025 AWS Well-Architected Framework 3 - EC2 Auto Scaling: + Launch Templates + Scaling Policies + Health Checks + Lifecycle Hooks 26/10/2025 26/10/2025 AWS Documentation 4 - Compute Optimization: + AWS Lambda (serverless) + AWS Fargate (serverless containers) + Lambda vs Fargate vs EC2 comparison 27/10/2025 27/10/2025 AWS Lambda Docs 5 - Performance Storage: + S3 performance tuning + EBS vs EFS + Throughput \u0026amp; IOPS optimization + Caching with DAX/ElastiCache 28/10/2025 28/10/2025 AWS Storage Blog 6 - Global Content Delivery: + CloudFront + Caching strategies + Global Accelerator - Practice: Configure CloudFront + S3 29/10/2025 29/10/2025 https://aws.amazon.com/cloudfront/ Week 6 Achievements: Gained a solid understanding of High-Performing Architectures and AWS optimization principles. Learned and configured EC2 Auto Scaling with scaling policies, lifecycle hooks, and health checks. Compared Lambda, Fargate, and EC2 to choose the optimal compute option for different workloads. Improved storage performance using: S3 (multipart uploads, prefix optimization) EBS (IO1/GP3, IOPS tuning) EFS (bursting and throughput modes) Understood and implemented caching using CloudFront, ElastiCache, and DynamoDB DAX. Successfully configured CloudFront for global content distribution using an S3 backend. Understood how Global Accelerator improves routing, latency, and availability. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nThis section contains my personal reflections during the First Cloud Journey internship program. These insights aim to support FCJ in improving and enhancing the program in the future.\nOverall Evaluation 1. Working Environment\nThe working environment at FCJ is friendly, open, and highly supportive. Team members are always willing to help whenever I encounter difficulties, even outside regular working hours. The workspace is organized and comfortable, which greatly improves my focus. However, I believe FCJ could consider organizing more team bonding activities to strengthen relationships among members.\n2. Support from Mentor / Team Admin\nMy mentor provides very detailed guidance, explains concepts clearly, and consistently encourages me to ask questions when I do not fully understand something. The admin team also supports all administrative matters effectively and provides the necessary materials for my work. I especially appreciate that my mentor allows me to explore and solve problems independently instead of giving me the answer immediately.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned were closely related to what I learned at university. At the same time, I was introduced to new areas that expanded my technical understanding and helped me strengthen both foundational and practical skills.\n4. Opportunities for Learning \u0026amp; Skill Development\nThroughout the internship, I gained valuable skills such as using project management tools, collaborating with a team, and communicating professionally in a corporate environment. My mentor also shared practical industry experiences, which helped me better plan my future career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone is respectful, works seriously, yet maintains a pleasant atmosphere. When urgent tasks arise, the entire team works together regardless of their roles or seniority. This made me feel like a real member of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. Being able to join internal training sessions is also a major benefit that helps me build additional knowledge.\nAdditional Questions ‚Äî Answers 1. What did I find most satisfying during the internship?\nWhat I appreciated most was the opportunity to gain real-world Cloud knowledge and the dedicated guidance from my mentor. I learned how to think systematically, ask the right questions, and approach issues independently‚Äîskills that are essential for my future career. The supportive environment also motivated me to keep improving each day.\n2. What should the company improve for future interns?\nFrom my perspective, the program is already well structured. However, FCJ could consider adding:\nCareer orientation workshops led by experienced professionals More group-based exercises or simulated real projects A fixed weekly mentoring schedule to help interns track progress more effectively These improvements could further enhance the overall learning experience.\n3. Would I recommend this internship to a friend? Why or why not?\nAbsolutely yes.\nFCJ offers a positive environment, dedicated mentors, clear learning pathways, and exposure to modern technologies. It is an excellent place for anyone aspiring to pursue Cloud or DevOps.\nSuggestions \u0026amp; Expectations ‚Äî Answers 1. Suggestions for improving the internship experience\nI would like to propose a few ideas:\nBuilding a centralized learning hub for easy access to all study materials Adding topic-focused mini-projects to immediately apply new knowledge Incorporating regular two-way feedback sessions between mentors and interns These additions can make the learning experience more practical and engaging.\n2. Expectations for continuing with the program\nIf given the opportunity, I would be happy to continue participating in future phases of FCJ. I hope to be involved in real projects and develop deeper expertise in areas such as Cloud Architecture, Monitoring, Security, or Automation.\n3. Additional comments\nMy time at FCJ not only improved my technical skills but also helped me clarify my long-term career direction. I truly appreciate the support and guidance from the FCJ team throughout my internship journey.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 7 Objectives: Understand the principles of Cost-Optimized Architectures on AWS. Learn how to analyze AWS spending using Cost Explorer. Set up AWS Budgets to monitor and control costs with alerts. Study Savings Plans and compare their cost-saving benefits. Learn how to optimize costs for NAT Gateway and reduce data transfer. Apply S3 Lifecycle Policies and Storage Tiering to minimize storage cost. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn Cost Explorer basics - Analyze historical cost \u0026amp; service breakdown - Identify top cost drivers 30/10/2025 https://docs.aws.amazon.com/cost-management/latest/userguide/ce-what-is.html 3 - Create AWS Budgets (Cost \u0026amp; Usage) - Configure email alerts - Track cost anomalies 31/10/2025 https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html 4 - Learn about Savings Plans (Compute SP, EC2 SP) - Compare On-Demand vs Savings Plans vs Reserved Instances - Use SP Calculator to estimate savings 01/11/2025 https://aws.amazon.com/savingsplans/ 5 - NAT Gateway cost optimization: + Reduce cross-AZ traffic + Use VPC Endpoints/PrivateLink + Consolidate NAT usage 02/11/2025 https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html 6 - Learn S3 Lifecycle Policies - Apply Storage Tiering (Standard ‚Üí IA ‚Üí Glacier) - Analyze cost savings from lifecycle transitions 03/11/2025 https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html Week 7 Achievements: Gained a clear understanding of Cost Explorer and learned how to identify major cost contributors, usage patterns, and monthly spending trends.\nSuccessfully created AWS Budgets with automated alerts to track cost thresholds and prevent overspending.\nUnderstood the differences between Savings Plans, Reserved Instances, and On-Demand, including how Savings Plans deliver long-term cost reduction.\nAnalyzed NAT Gateway usage and learned multiple ways to optimize cost, such as minimizing cross-AZ traffic and using VPC Endpoints.\nPracticed configuring S3 Lifecycle Policies to automatically transition objects between storage tiers (Standard ‚Üí Standard-IA ‚Üí Glacier), reducing long-term storage cost.\nStrengthened the ability to design cloud architectures focused on cost efficiency, balancing performance and budget.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only.\nPlease do not copy verbatim for your own report, including this warning.\nWeek 8 Objectives: Understand the AWS Well-Architected Framework and its six pillars. Strengthen knowledge of core AWS services: EC2, S3, IAM, RDS, VPC, Lambda, CloudWatch, CloudFront. Learn to apply best practices when designing cloud architectures. Combine Well-Architected mindset with real design scenarios. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduction to AWS Well-Architected Framework - Study 6 pillars: + Operational Excellence + Security + Reliability + Performance Efficiency + Cost Optimization + Sustainability 04/11/2025 https://aws.amazon.com/architecture/well-architected 3 - Deep dive into EC2 + Instance Types + AMI + EBS + Security Groups - Analyze workload‚Äìinstance matching 05/11/2025 https://docs.aws.amazon.com/ec2 4 - Learn S3: + Storage Classes + Versioning + Lifecycle Rules - Practice creating bucket \u0026amp; uploading files 06/11/2025 https://docs.aws.amazon.com/s3 5 - Study IAM: + Users, Groups, Roles + IAM Policies (JSON) + Least Privilege Principle - Practice creating IAM User/Role and assigning permissions 07/11/2025 https://docs.aws.amazon.com/iam 6 - Deep dive into RDS, VPC, Lambda, CloudWatch, CloudFront: + RDS: Multi-AZ, Backups + VPC: Subnets, Route Tables, IGW, NAT + Lambda: Event-driven compute + CloudWatch: Metrics, Logs, Alarms + CloudFront: CDN, Edge Locations - Practice designing an integrated architecture using these services 08/11/2025 AWS Documentation Week 8 Achievements: Understood all 6 pillars of AWS Well-Architected Framework and how they impact system design. Strengthened EC2 knowledge and learned to choose the right instance for each workload. Practiced S3 Versioning, Storage Classes, and Lifecycle Policies. Mastered IAM best practices: least privilege, roles, policy design. Mapped and simulated architecture with VPC, RDS, Lambda, CloudWatch, and CloudFront. Demonstrated ability to evaluate and optimize architecture based on AWS best practices. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only.\nPlease do not copy verbatim for your own report, including this warning.\nWeek 9 Objectives: Strengthen core mathematical foundations for ML/DL: Linear Algebra, Probability, Statistics, Calculus. Review Python fundamentals: data structures, file handling, exception handling, OOP. Learn UI building and web applications for ML deployment. Understand REST APIs, async programming, and model deployment workflows. Study ML tasks: Classification, Regression, Pipelines, and Feature Engineering. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn Math for ML/DL: + Linear Algebra (vectors, matrices, dot product) + Probability (random variables, distributions) + Statistics (mean, variance, correlation) + Calculus (derivatives, gradients) 09/11/2025 https://developers.google.com/machine-learning/crash-course 3 - Study Python fundamentals: + Lists, tuples, sets, dicts + File handling + Exception handling - Understand OOP: classes, objects, inheritance, polymorphism 10/11/2025 https://docs.python.org 4 - Learn UI building for ML: + Streamlit basics + Creating ML dashboards - Build simple interactive ML UI 11/11/2025 https://streamlit.io 5 - Study Web apps for ML + Flask/FastAPI + REST APIs (GET/POST) + Async programming basics - Build simple ML API endpoint 12/11/2025 https://fastapi.tiangolo.com 6 - ML concepts: + Classification vs Regression + Scikit-learn Pipelines + Feature Engineering: scaling, encoding, selection - Train a small ML pipeline end-to-end 13/11/2025 https://scikit-learn.org Week 9 Achievements: Gained strong mathematical foundations for machine learning, especially in linear algebra, probability, and calculus. Reviewed essential Python skills: data structures, OOP, file handling, and exception handling. Built UI applications for ML using Streamlit. Created REST API endpoints using Flask/FastAPI and explored async workflows. Understood and practiced ML tasks including classification, regression, pipelines, and feature engineering. Developed the ability to build and deploy small ML applications from scratch. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 10 Objectives: Understand how to work with Foundation Models on Amazon Bedrock. Learn how to build agents using Bedrock. Implement the RAG (Retrieval-Augmented Generation) pattern. Explore AI-powered search and conversational applications. Study Multi-Modal Foundation Models. Understand fine-tuning LLMs with Amazon SageMaker. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Overview of Foundation Models in Amazon Bedrock - Study model categories (Text, Embeddings, Multimodal, Agents) 14/11/2025 14/11/2025 AWS Docs 3 - Learn Building Agents with Amazon Bedrock - Study Agent architecture, orchestration, action groups 15/11/2025 15/11/2025 AWS Bedrock 4 - Implement RAG Pattern - Understanding Vector DB, embeddings, retriever, generator 16/11/2025 16/11/2025 RAG Docs 5 - Build AI-powered Search \u0026amp; Conversational Applications - Learn grounding, context injection, query rewriting 17/11/2025 17/11/2025 AWS Bedrock 6 - Study Multi-Modal Foundation Models - Overview of image, text, speech integration - Learn Fine-Tuning LLMs with Amazon SageMaker 18/11/2025 18/11/2025 AWS SageMaker Docs Week 10 Achievements: Gained a solid understanding of Foundation Models available on Amazon Bedrock. Learned how to build Agents and integrate Action Groups for external tool execution. Completed a full RAG pipeline using embeddings + vector store + retriever + generator. Built a prototype conversational application with context-aware responses. Explored multimodal models capable of handling text, images, and more. Understood the fine-tuning workflow on Amazon SageMaker: Dataset preparation Training jobs Evaluation \u0026amp; deployment "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 11 Objectives: Learn how to build a serverless backend using AWS Lambda, S3, and DynamoDB. Understand frontend development for serverless APIs. Automate deployment using AWS SAM. Implement user authentication with Amazon Cognito. Configure custom domains and SSL certificates for serverless applications. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study Serverless Backend Architecture - Learn Lambda triggers, IAM roles, S3 events, DynamoDB operations 19/11/2025 19/11/2025 AWS Docs 3 - Build Serverless Backend with Lambda, S3, DynamoDB - Implement CRUD API using Lambda + DynamoDB 20/11/2025 20/11/2025 AWS Workshop 4 - Learn Frontend Development for Serverless APIs - API Gateway + Lambda integration - CORS, stages, routes 21/11/2025 21/11/2025 API Gateway Docs 5 - Study AWS SAM (Serverless Application Model) - Automate deployment - Write SAM templates - Run sam build, sam deploy 22/11/2025 22/11/2025 AWS SAM Docs 6 - Implement User Authentication with Amazon Cognito - User Pools, App Clients, Hosted UI - Integrate Cognito with API Gateway - Learn Custom Domains \u0026amp; SSL via ACM 23/11/2025 23/11/2025 Cognito Docs Week 11 Achievements: Built a complete serverless backend with Lambda, S3, and DynamoDB (Book Store use case). Created APIs using API Gateway integrated with Lambda (CRUD operations). Implemented a lightweight frontend workflow for serverless APIs. Successfully used AWS SAM to automate deployment steps. Configured Amazon Cognito for user authentication and API protection. Explored how to set up custom domains and SSL for serverless applications using Amazon CloudFront \u0026amp; ACM. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 12 Objectives: Build a Serverless Document Management System. Implement CRUD operations using Lambda and DynamoDB. Integrate Serverless Storage and Authentication with AWS Amplify. Connect frontend with APIs via API Gateway. Deploy Document Management System using AWS SAM. Set up Content Delivery using Amazon CloudFront. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study Serverless CRUD with Lambda and DynamoDB - Lambda functions, DynamoDB tables, IAM policies 24/11/2025 24/11/2025 AWS Docs 3 - Implement CRUD operations for Document Management System - Upload, update, delete, query documents 25/11/2025 25/11/2025 AWS Workshop 4 - Learn Serverless Storage \u0026amp; Authentication with AWS Amplify - Amplify Auth, Storage, Hosting 26/11/2025 26/11/2025 AWS Amplify Docs 5 - Integrate frontend with API Gateway - Routes, methods, request/response mapping - CORS configuration 27/11/2025 27/11/2025 API Gateway Docs 6 - Deploy entire Document Management System using AWS SAM - Configure CloudFront for content delivery 28/11/2025 28/11/2025 AWS SAM + CloudFront Week 12 Achievements: Built a full serverless CRUD system using Lambda + DynamoDB. Added serverless storage and authentication through AWS Amplify. Successfully integrated frontend with backend using API Gateway. Automated deployment using AWS SAM templates. Configured CloudFront to deliver the application globally with caching and SSL support. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]